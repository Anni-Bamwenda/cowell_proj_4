{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Tweet Collection Process"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "# Import the usual suspects\n",
    "import json\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Help get those tweets\n",
    "from tweepy.streaming import StreamListener\n",
    "from tweepy import OAuthHandler, Stream, API\n",
    "import tweepy\n",
    "\n",
    "# Test the pipeline\n",
    "from pipeline import NLPPipe, tweet_clean1\n",
    "\n",
    "# Turn the JSON data into a DataFrame\n",
    "from helper_functions import txt_to_df, display_topics, scatter\n",
    "# import helper_functions\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Necessary details to access Twitter API\n",
    "access_token = \"1921879345-ZWORqpOrCvBNlYarCaJucUrwYK5kDGP9eckgMHI\"\n",
    "access_token_secret = \"ucVJX0dkXywIDsRNBa2VfQlY7oZCLk0zlrVSVbOPA7vDv\"\n",
    "consumer_key = \"vkkBaYfbVIx6PFvYT3uFH3tXh\"\n",
    "consumer_secret = \"7h7owpn6rrzhHqxNQHjcYn3etCew8CqanLCQxkhSyELMvBDX6S\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up Twitter access authentication\n",
    "auth = OAuthHandler(consumer_key, consumer_secret)\n",
    "auth.set_access_token(access_token, access_token_secret)\n",
    "api = API(auth)"
   ]
  },
  {
   "source": [
    "The code below conducts the search and stores results as JSON. Since it is searching for 5,000 items, the application will eventually get blocked due to Twitter limitations.\n",
    "I will run this cell with various key words (vegan, veganism, plant based)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "TweepError",
     "evalue": "Twitter error response: status code = 429",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTweepError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-f87b30cb90e5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0msearch_results\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mstatus\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtweepy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCursor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mapi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msearch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mq\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'veganism'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtweet_mode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'extended'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m5000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0msearch_results\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstatus\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_json\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'search_results.json'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'w'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/tweepy/cursor.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__next__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/tweepy/cursor.py\u001b[0m in \u001b[0;36mnext\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    217\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcurrent_page\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpage_index\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcurrent_page\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m             \u001b[0;31m# Reached end of current page, get the next page...\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 219\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcurrent_page\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpage_iterator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    220\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcurrent_page\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    221\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcurrent_page\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpage_iterator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/tweepy/cursor.py\u001b[0m in \u001b[0;36mnext\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    128\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresults\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 130\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_id\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparser\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mRawParser\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    131\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    132\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'__self__'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/tweepy/binder.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    250\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    251\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 252\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    253\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    254\u001b[0m             \u001b[0mmethod\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/tweepy/binder.py\u001b[0m in \u001b[0;36mexecute\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    232\u001b[0m                     \u001b[0;32mraise\u001b[0m \u001b[0mRateLimitError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merror_msg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    233\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 234\u001b[0;31m                     \u001b[0;32mraise\u001b[0m \u001b[0mTweepError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merror_msg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mapi_code\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mapi_error_code\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    235\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    236\u001b[0m             \u001b[0;31m# Parse the response payload\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTweepError\u001b[0m: Twitter error response: status code = 429"
     ]
    }
   ],
   "source": [
    "search_results = []\n",
    "for status in tweepy.Cursor(api.search, q='veganism', tweet_mode='extended').items(5000):\n",
    "    search_results.append(status._json)\n",
    "\n",
    "with open('search_results.json', 'w') as f:\n",
    "    json.dump(search_results, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterate through tweets and save to a .txt file for easy retrieval\n",
    "for tweet in search_results:\n",
    "    saveFile = open('twitter_data7.txt', 'a')\n",
    "    saveFile.write(json.dumps(tweet))\n",
    "    saveFile.write('\\n')\n",
    "    saveFile.write('\\n')\n",
    "saveFile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load those txt files as DataFrames\n",
    "tweets2 = txt_to_df(txt_path='Data/twitter_data2.txt')\n",
    "tweets3 = txt_to_df(txt_path='Data/twitter_data3.txt')\n",
    "tweets4 = txt_to_df(txt_path='Data/twitter_data4.txt')\n",
    "tweets5 = txt_to_df(txt_path='Data/twitter_data5.txt')\n",
    "tweets6 = txt_to_df(txt_path='Data/twitter_data6.txt')\n",
    "tweets7 = txt_to_df(txt_path='Data/twitter_data7.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I used snscrape to pull more tweets, let's get those going\n",
    "tw2015 = txt_to_df('Data/tw2015.txt')\n",
    "tw2016 = txt_to_df('Data/tw2016.txt')\n",
    "tw2017 = txt_to_df('Data/tw2017.txt')\n",
    "tw2018 = txt_to_df('Data/tw2018.txt')\n",
    "tw2019 = txt_to_df('Data/tw2019.txt')\n",
    "tw2020 = txt_to_df('Data/tw2020.txt')\n",
    "t2w2015 = txt_to_df('Data/t2w2015.txt')\n",
    "t2w2016 = txt_to_df('Data/t2w2016.txt')\n",
    "t2w2017 = txt_to_df('Data/t2w2017.txt')\n",
    "t2w2018 = txt_to_df('Data/t2w2018.txt')\n",
    "t2w2019 = txt_to_df('Data/t2w2019.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine all of the DataFrames into the full Tweet corpus\n",
    "all_tweets = pd.concat([tweets2, tweets3, tweets4, tweets5, tweets6, tweets7,tw2015,tw2016,tw2017,tw2018,tw2019,tw2020,t2w2015,t2w2016,t2w2017,t2w2018,t2w2019])\n",
    "all_tweets = all_tweets.drop_duplicates().reset_index()\n",
    "all_tweets.drop(labels='index',axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_tweets.to_pickle(\"tweets_year2.pkl\")\n",
    "# Now we save this DataFrame for further processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "                                               long_text  year\n",
       "0      RT @ForksOverKnives: Fall colors shine through...  2020\n",
       "1      Risotto, daal, pasta bake with herbs, a pumpki...  2020\n",
       "2      @queennjemima You say that now but there’s a l...  2020\n",
       "3                                            Good read    2020\n",
       "4      RT @Fiorella_im: BREAKING according to judge, ...  2020\n",
       "...                                                  ...   ...\n",
       "58477  Instagram’s Official Account Promotes 🌱 Activi...  2019\n",
       "58478  Chillin’ the fuck out! @sundayscaries #sundays...  2019\n",
       "58479  Made this tonight to go with a new EASY #recip...  2019\n",
       "58480  #Repost kashiwaramen with get_repost\\n・・・\\nLoo...  2019\n",
       "58481  I was never a big fan of Tofu, but when done r...  2019\n",
       "\n",
       "[58482 rows x 2 columns]"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>long_text</th>\n      <th>year</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>RT @ForksOverKnives: Fall colors shine through...</td>\n      <td>2020</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Risotto, daal, pasta bake with herbs, a pumpki...</td>\n      <td>2020</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>@queennjemima You say that now but there’s a l...</td>\n      <td>2020</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Good read</td>\n      <td>2020</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>RT @Fiorella_im: BREAKING according to judge, ...</td>\n      <td>2020</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>58477</th>\n      <td>Instagram’s Official Account Promotes 🌱 Activi...</td>\n      <td>2019</td>\n    </tr>\n    <tr>\n      <th>58478</th>\n      <td>Chillin’ the fuck out! @sundayscaries #sundays...</td>\n      <td>2019</td>\n    </tr>\n    <tr>\n      <th>58479</th>\n      <td>Made this tonight to go with a new EASY #recip...</td>\n      <td>2019</td>\n    </tr>\n    <tr>\n      <th>58480</th>\n      <td>#Repost kashiwaramen with get_repost\\n・・・\\nLoo...</td>\n      <td>2019</td>\n    </tr>\n    <tr>\n      <th>58481</th>\n      <td>I was never a big fan of Tofu, but when done r...</td>\n      <td>2019</td>\n    </tr>\n  </tbody>\n</table>\n<p>58482 rows × 2 columns</p>\n</div>"
     },
     "metadata": {},
     "execution_count": 19
    }
   ],
   "source": [
    "all_tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'Data/twitter_data2.txt'\n",
    "tweets_file = open(path, \"r\")\n",
    "tweets_data = []\n",
    "for line in tweets_file:\n",
    "    try:\n",
    "        tweet = json.loads(line)\n",
    "        tweets_data.append(tweet)\n",
    "    except:\n",
    "        continue\n",
    "tweet = pd.DataFrame(tweets_data)\n",
    "tweet = tweet[tweet[\"lang\"] == \"en\"]\n",
    "tweet = tweet[[\"full_text\",\"created_at\",\"retweeted_status\"]]\n",
    "tweet[\"long_text\"] = tweet[\"full_text\"]\n",
    "tweet[\"long_text\"] = tweet.apply(ext_rt, axis=1)\n",
    "# tweet[\"year\"] = pd.to_datetime(tweet[\"created_at\"])\n",
    "# tweet[\"year\"] = tweet.apply(to_year, axis=1)\n",
    "# tweet[[\"long_text\", \"year\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "                                              full_text  \\\n",
       "1     RT @ForksOverKnives: Fall colors shine through...   \n",
       "2     Risotto, daal, pasta bake with herbs, a pumpki...   \n",
       "6     @queennjemima You say that now but there’s a l...   \n",
       "7                    Good read  https://t.co/NLKwgnZNGe   \n",
       "8     RT @Fiorella_im: BREAKING according to judge, ...   \n",
       "...                                                 ...   \n",
       "2668  Running with Don with some stops for drinking,...   \n",
       "2670  RT @Sonic1938: \"To all suffering. To the slaug...   \n",
       "2673  @SixbyNico the #vegan #home #delivery is inspi...   \n",
       "2674  RT @Cat_Kapow: Remembering the magnificent hor...   \n",
       "2676  @PETAUK’s giant ‘Leek’ brings her ‘Leeks not L...   \n",
       "\n",
       "                          created_at  \\\n",
       "1     Tue Nov 03 16:06:43 +0000 2020   \n",
       "2     Tue Nov 03 16:06:42 +0000 2020   \n",
       "6     Tue Nov 03 16:06:32 +0000 2020   \n",
       "7     Tue Nov 03 16:06:30 +0000 2020   \n",
       "8     Tue Nov 03 16:06:29 +0000 2020   \n",
       "...                              ...   \n",
       "2668  Tue Nov 03 14:01:35 +0000 2020   \n",
       "2670  Tue Nov 03 14:01:26 +0000 2020   \n",
       "2673  Tue Nov 03 14:01:17 +0000 2020   \n",
       "2674  Tue Nov 03 14:01:17 +0000 2020   \n",
       "2676  Tue Nov 03 14:01:13 +0000 2020   \n",
       "\n",
       "                                       retweeted_status  \\\n",
       "1     {'created_at': 'Tue Nov 03 16:02:10 +0000 2020...   \n",
       "2                                                   NaN   \n",
       "6                                                   NaN   \n",
       "7                                                   NaN   \n",
       "8     {'created_at': 'Tue Nov 03 15:43:09 +0000 2020...   \n",
       "...                                                 ...   \n",
       "2668                                                NaN   \n",
       "2670  {'created_at': 'Sat Oct 31 11:57:49 +0000 2020...   \n",
       "2673                                                NaN   \n",
       "2674  {'created_at': 'Mon Nov 02 13:55:03 +0000 2020...   \n",
       "2676                                                NaN   \n",
       "\n",
       "                                              long_text  \n",
       "1     Fall colors shine through in this sweet and ta...  \n",
       "2     Risotto, daal, pasta bake with herbs, a pumpki...  \n",
       "6     @queennjemima You say that now but there’s a l...  \n",
       "7                    Good read  https://t.co/NLKwgnZNGe  \n",
       "8     BREAKING according to judge, NO EVIDENCE FOUND...  \n",
       "...                                                 ...  \n",
       "2668  Running with Don with some stops for drinking,...  \n",
       "2670  \"To all suffering. To the slaughtered. To the ...  \n",
       "2673  @SixbyNico the #vegan #home #delivery is inspi...  \n",
       "2674  Remembering the magnificent horses that have d...  \n",
       "2676  @PETAUK’s giant ‘Leek’ brings her ‘Leeks not L...  \n",
       "\n",
       "[2070 rows x 4 columns]"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>full_text</th>\n      <th>created_at</th>\n      <th>retweeted_status</th>\n      <th>long_text</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>1</th>\n      <td>RT @ForksOverKnives: Fall colors shine through...</td>\n      <td>Tue Nov 03 16:06:43 +0000 2020</td>\n      <td>{'created_at': 'Tue Nov 03 16:02:10 +0000 2020...</td>\n      <td>Fall colors shine through in this sweet and ta...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Risotto, daal, pasta bake with herbs, a pumpki...</td>\n      <td>Tue Nov 03 16:06:42 +0000 2020</td>\n      <td>NaN</td>\n      <td>Risotto, daal, pasta bake with herbs, a pumpki...</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>@queennjemima You say that now but there’s a l...</td>\n      <td>Tue Nov 03 16:06:32 +0000 2020</td>\n      <td>NaN</td>\n      <td>@queennjemima You say that now but there’s a l...</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>Good read  https://t.co/NLKwgnZNGe</td>\n      <td>Tue Nov 03 16:06:30 +0000 2020</td>\n      <td>NaN</td>\n      <td>Good read  https://t.co/NLKwgnZNGe</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>RT @Fiorella_im: BREAKING according to judge, ...</td>\n      <td>Tue Nov 03 16:06:29 +0000 2020</td>\n      <td>{'created_at': 'Tue Nov 03 15:43:09 +0000 2020...</td>\n      <td>BREAKING according to judge, NO EVIDENCE FOUND...</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>2668</th>\n      <td>Running with Don with some stops for drinking,...</td>\n      <td>Tue Nov 03 14:01:35 +0000 2020</td>\n      <td>NaN</td>\n      <td>Running with Don with some stops for drinking,...</td>\n    </tr>\n    <tr>\n      <th>2670</th>\n      <td>RT @Sonic1938: \"To all suffering. To the slaug...</td>\n      <td>Tue Nov 03 14:01:26 +0000 2020</td>\n      <td>{'created_at': 'Sat Oct 31 11:57:49 +0000 2020...</td>\n      <td>\"To all suffering. To the slaughtered. To the ...</td>\n    </tr>\n    <tr>\n      <th>2673</th>\n      <td>@SixbyNico the #vegan #home #delivery is inspi...</td>\n      <td>Tue Nov 03 14:01:17 +0000 2020</td>\n      <td>NaN</td>\n      <td>@SixbyNico the #vegan #home #delivery is inspi...</td>\n    </tr>\n    <tr>\n      <th>2674</th>\n      <td>RT @Cat_Kapow: Remembering the magnificent hor...</td>\n      <td>Tue Nov 03 14:01:17 +0000 2020</td>\n      <td>{'created_at': 'Mon Nov 02 13:55:03 +0000 2020...</td>\n      <td>Remembering the magnificent horses that have d...</td>\n    </tr>\n    <tr>\n      <th>2676</th>\n      <td>@PETAUK’s giant ‘Leek’ brings her ‘Leeks not L...</td>\n      <td>Tue Nov 03 14:01:13 +0000 2020</td>\n      <td>NaN</td>\n      <td>@PETAUK’s giant ‘Leek’ brings her ‘Leeks not L...</td>\n    </tr>\n  </tbody>\n</table>\n<p>2070 rows × 4 columns</p>\n</div>"
     },
     "metadata": {},
     "execution_count": 38
    }
   ],
   "source": [
    "tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ext_rt(row):\n",
    "    try:\n",
    "        if type(row[\"retweeted_status\"]) == dict:\n",
    "            return row[\"retweeted_status\"][\"full_text\"]\n",
    "        else:\n",
    "            return row[\"long_text\"]\n",
    "    except:\n",
    "        return row[\"long_text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'Fall colors shine through in this sweet and tangy Roasted Carrot and Fennel Salad with homemade dressing. https://t.co/GmbvfBfUY3 https://t.co/dzgmBxzYya'"
      ]
     },
     "metadata": {},
     "execution_count": 21
    }
   ],
   "source": [
    "tweet.iloc[1,:]['retweeted_status']['full_text']\n",
    "# I NEED TO UPDATE THE FUNCTION AGAIN TO WORK WITH RETWEETS!!!!!!!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "source": [
    "# Improve the function!!"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "tw2016 = txt_to_df('Data/tw2016.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "                                              long_text  year\n",
       "0     Happy birthday bubbles &lt;3 #birthday #birthd...  2016\n",
       "2                         #Vegan Life @ Cafe Sunflower   2016\n",
       "3     The Best Natural Personal Care Products For Yo...  2016\n",
       "4     We have something special for you tomorrow! We...  2016\n",
       "5           #Raw #Vegan Tropical Creamsicle #Smoothie    2016\n",
       "...                                                 ...   ...\n",
       "4994  PETA’s Best Vegan Food List of 2016:  #vegan #...  2016\n",
       "4995  Throw Back Thursday..Vegan Spicy Hummus and Cu...  2016\n",
       "4996  Right proper #vegan #menu changed daily - @chr...  2016\n",
       "4998  ORDER THE BEST HOMEMADE VEGAN BANANA PUDDING 😋...  2016\n",
       "4999  Hilarious new name. I love the range of coconu...  2016\n",
       "\n",
       "[3765 rows x 2 columns]"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>long_text</th>\n      <th>year</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Happy birthday bubbles &amp;lt;3 #birthday #birthd...</td>\n      <td>2016</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>#Vegan Life @ Cafe Sunflower</td>\n      <td>2016</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>The Best Natural Personal Care Products For Yo...</td>\n      <td>2016</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>We have something special for you tomorrow! We...</td>\n      <td>2016</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>#Raw #Vegan Tropical Creamsicle #Smoothie</td>\n      <td>2016</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>4994</th>\n      <td>PETA’s Best Vegan Food List of 2016:  #vegan #...</td>\n      <td>2016</td>\n    </tr>\n    <tr>\n      <th>4995</th>\n      <td>Throw Back Thursday..Vegan Spicy Hummus and Cu...</td>\n      <td>2016</td>\n    </tr>\n    <tr>\n      <th>4996</th>\n      <td>Right proper #vegan #menu changed daily - @chr...</td>\n      <td>2016</td>\n    </tr>\n    <tr>\n      <th>4998</th>\n      <td>ORDER THE BEST HOMEMADE VEGAN BANANA PUDDING 😋...</td>\n      <td>2016</td>\n    </tr>\n    <tr>\n      <th>4999</th>\n      <td>Hilarious new name. I love the range of coconu...</td>\n      <td>2016</td>\n    </tr>\n  </tbody>\n</table>\n<p>3765 rows × 2 columns</p>\n</div>"
     },
     "metadata": {},
     "execution_count": 12
    }
   ],
   "source": [
    "tw2016.drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "These two functions help take twitter data and turn it into a\n",
    "workable corpus for NLP\n",
    "\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import json\n",
    "\n",
    "\n",
    "def txt_to_df(txt_path):\n",
    "    \"\"\"\n",
    "    Powerhouse function to take raw scraped twitter data\n",
    "    into a DataFrame of just the tweet texts\n",
    "\n",
    "    Args:\n",
    "        txt_path: The path for the saved file containing\n",
    "            the tweet data in json format\n",
    "\n",
    "    Returns:\n",
    "        A DataFrame containing just the raw tweet texts\n",
    "    \"\"\"\n",
    "    path = txt_path\n",
    "    tweets_file = open(path, \"r\")\n",
    "    tweets_data = []\n",
    "    for line in tweets_file:\n",
    "        try:\n",
    "            tweet = json.loads(line)\n",
    "            tweets_data.append(tweet)\n",
    "        except:\n",
    "            continue\n",
    "    tweet = pd.DataFrame(tweets_data)\n",
    "    tweet = tweet[tweet[\"lang\"] == \"en\"]\n",
    "    try:\n",
    "        tweet = tweet[[\"full_text\",\"created_at\",\"retweeted_status\"]]\n",
    "        tweet[\"long_text\"] = tweet[\"full_text\"]\n",
    "        tweet[\"long_text\"] = tweet.apply(ext_rt, axis=1)\n",
    "        tweet[\"long_text\"] = tweet.apply(rm_links, axis=1)\n",
    "        tweet[\"year\"] = pd.to_datetime(tweet[\"created_at\"])\n",
    "        tweet[\"year\"] = tweet.apply(to_year, axis=1)\n",
    "        return tweet[[\"long_text\", \"year\"]]\n",
    "    except:\n",
    "        try:\n",
    "            tweet[\"long_text\"] = tweet['content']\n",
    "            tweet[\"long_text\"] = tweet.apply(rm_links, axis=1)\n",
    "            tweet[\"year\"] = pd.to_datetime(tweet[\"date\"])\n",
    "            tweet[\"year\"] = tweet.apply(to_year, axis=1)\n",
    "            return tweet[[\"long_text\", \"year\"]]\n",
    "        except:\n",
    "            return tweet[[\"long_text\", \"year\"]]\n",
    "\n",
    "\n",
    "def snscrape_to_df(txt_path):\n",
    "    path = txt_path\n",
    "    tweets_file = open(path, \"r\")\n",
    "    tweets_data = []\n",
    "    for line in tweets_file:\n",
    "        try:\n",
    "            tweet = json.loads(line)\n",
    "            tweets_data.append(tweet)\n",
    "        except:\n",
    "            continue\n",
    "    tweet = pd.DataFrame(tweets_data)\n",
    "    tweet = tweet[tweet[\"lang\"] == \"en\"]\n",
    "    tweet[\"long_text\"] = tweet[\"content\"]\n",
    "    return tweet[[\"long_text\"]]\n",
    "\n",
    "def ext_rt(row):\n",
    "    try:\n",
    "        if type(row[\"retweeted_status\"]) == dict:\n",
    "            return row[\"retweeted_status\"][\"extended_tweet\"][\"full_text\"]\n",
    "        else:\n",
    "            return row[\"long_text\"]\n",
    "    except:\n",
    "        return row[\"long_text\"]\n",
    "\n",
    "def to_year(row):\n",
    "    return row[\"year\"].year\n",
    "\n",
    "def rm_links(row):\n",
    "    text = row[\"long_text\"]\n",
    "    text = re.sub(r'https:\\S*', '', text)\n",
    "    row[\"long_text\"] = text\n",
    "    return row[\"long_text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}